{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2021/22\n",
    "\n",
    "### Practice - Implicit Alternating Least Squares\n",
    "\n",
    "See:\n",
    "Y. Hu, Y. Koren and C. Volinsky, Collaborative filtering for implicit feedback datasets, ICDM 2008.\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.5120&rep=rep1&type=pdf\n",
    "\n",
    "R. Pan et al., One-class collaborative filtering, ICDM 2008.\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.306.4684&rep=rep1&type=pdf\n",
    "\n",
    "Factorization model for binary feedback.\n",
    "First, splits the feedback matrix R as the element-wise a Preference matrix P and a Confidence matrix C.\n",
    "Then computes the decomposition of them into the dot product of two matrices X and Y of latent factors.\n",
    "X represent the user latent factors, Y the item latent factors.\n",
    "\n",
    "The model is learned by solving the following regularized Least-squares objective function with Stochastic Gradient Descent\n",
    "    \n",
    "$$\\frac{1}{2}\\sum_{i,j}{c_{ij}\\left(p_{ij}-x_i^T y_j\\right) + \\lambda\\left(\\sum_{i}{||x_i||^2} + \\sum_{j}{||y_j||^2}\\right)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10M: Verifying data consistency...\n",
      "Movielens10M: Verifying data consistency... Passed!\n",
      "DataReader: current dataset is: <class 'Data_manager.Dataset.Dataset'>\n",
      "\tNumber of items: 10681\n",
      "\tNumber of users: 69878\n",
      "\tNumber of interactions in URM_all: 10000054\n",
      "\tValue range in URM_all: 0.50-5.00\n",
      "\tInteraction density: 1.34E-02\n",
      "\tInteractions per user:\n",
      "\t\t Min: 2.00E+01\n",
      "\t\t Avg: 1.43E+02\n",
      "\t\t Max: 7.36E+03\n",
      "\tInteractions per item:\n",
      "\t\t Min: 0.00E+00\n",
      "\t\t Avg: 9.36E+02\n",
      "\t\t Max: 3.49E+04\n",
      "\tGini Index: 0.57\n",
      "\n",
      "\tICM name: ICM_all, Value range: 1.00 / 69.00, Num features: 10126, feature occurrences: 128384, density 1.19E-03\n",
      "\tICM name: ICM_genres, Value range: 1.00 / 1.00, Num features: 20, feature occurrences: 21564, density 1.01E-01\n",
      "\tICM name: ICM_tags, Value range: 1.00 / 69.00, Num features: 10106, feature occurrences: 106820, density 9.90E-04\n",
      "\tICM name: ICM_year, Value range: 6.00E+00 / 2.01E+03, Num features: 1, feature occurrences: 10681, density 1.00E+00\n",
      "\n",
      "\n",
      "Warning: 68 (0.10 %) of 69878 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "from Data_manager.Movielens.Movielens10MReader import Movielens10MReader\n",
    "\n",
    "data_reader = Movielens10MReader()\n",
    "data_loaded = data_reader.load_data()\n",
    "\n",
    "URM_all = data_loaded.get_URM_all()\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<69878x10681 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8000043 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we need for IALS?\n",
    "\n",
    "* User factor and Item factor matrices\n",
    "* Confidence function\n",
    "* Update rule for items\n",
    "* Update rule for users\n",
    "* Training loop and some patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_items = URM_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: We create the dense latent factor matrices\n",
    "### In a MF model you have two matrices, one with a row per user and the other with a column per item. The other dimension, columns for the first one and rows for the second one is called latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = 10\n",
    "\n",
    "user_factors = np.random.random((n_users, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.20851241e-01, 1.63905137e-01, 4.86327831e-01, ...,\n",
       "        2.21503420e-04, 3.04936016e-01, 2.17939140e-01],\n",
       "       [1.61363784e-01, 1.77579527e-01, 9.20555533e-01, ...,\n",
       "        1.23432443e-01, 6.79767136e-01, 5.99961364e-01],\n",
       "       [4.12273907e-02, 3.27771053e-02, 6.93553325e-01, ...,\n",
       "        1.67936854e-01, 9.13602434e-01, 5.34909302e-01],\n",
       "       ...,\n",
       "       [3.16940027e-01, 2.60197188e-01, 2.24889252e-01, ...,\n",
       "        3.27920286e-01, 1.33059415e-01, 5.92913342e-01],\n",
       "       [9.85818996e-01, 9.28862919e-01, 3.89115406e-01, ...,\n",
       "        4.33134217e-01, 8.95990763e-01, 9.46815934e-02],\n",
       "       [3.78013865e-01, 4.64941232e-01, 6.05416743e-01, ...,\n",
       "        5.53551360e-01, 7.17374978e-01, 1.72820114e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.39782363e-01, 5.39387513e-02, 5.98575358e-01, ...,\n",
       "        2.39586819e-01, 4.43841380e-01, 3.92436234e-01],\n",
       "       [4.72442733e-01, 9.73663491e-04, 9.88832309e-01, ...,\n",
       "        2.21326718e-01, 8.60111215e-01, 5.45528024e-02],\n",
       "       [1.90919694e-01, 3.78482189e-01, 6.15815880e-01, ...,\n",
       "        4.18085328e-01, 6.60688483e-01, 8.24092300e-01],\n",
       "       ...,\n",
       "       [7.73251115e-01, 7.72502439e-01, 5.00109383e-01, ...,\n",
       "        3.15539117e-01, 8.07099058e-01, 6.98021280e-02],\n",
       "       [6.78842766e-01, 7.61397470e-02, 4.86302848e-01, ...,\n",
       "        4.68098877e-01, 7.95784254e-01, 7.56779059e-01],\n",
       "       [1.37995912e-01, 6.10805938e-02, 1.68042426e-01, ...,\n",
       "        5.83592583e-01, 2.80148359e-01, 1.96091523e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: We define a function to transform the interaction data in a \"confidence\" value. \n",
    "* If you have explicit data, the higher it is the higher the confidence (logarithmic, linear?)\n",
    "* Other options include scaling the data lowering it if the item or use has very few interactions (lower support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_confidence_function(URM_train, alpha):\n",
    "    \n",
    "    URM_train.data = 1.0 + alpha*URM_train.data\n",
    "    \n",
    "    return URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "C_URM_train = linear_confidence_function(URM_train, alpha)\n",
    "\n",
    "C_URM_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of confidence can be defined in different ways, for example in terms of the number of interactions an item or a user has, the more they have the more support your model will have for the respective latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_confidence(URM_train):\n",
    "    \n",
    "    item_popularity = np.ediff1d(URM_train.tocsc().indptr)\n",
    "    item_confidence = np.zeros(len(item_popularity))\n",
    "    item_confidence[item_popularity!=0] = np.log(item_popularity[item_popularity!=0])\n",
    "    \n",
    "    C_URM_train = URM_train.copy()\n",
    "    C_URM_train = C_URM_train.tocsc()\n",
    "    \n",
    "    for item_id in range(C_URM_train.shape[1]):\n",
    "        start_pos = C_URM_train.indices[item_id]\n",
    "        end_pos = C_URM_train.indices[item_id+1]\n",
    "        \n",
    "        C_URM_train.data[start_pos:end_pos] = item_confidence[item_id]\n",
    "    \n",
    "    C_URM_train = C_URM_train.tocsr()\n",
    "    \n",
    "    return C_URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.89233654, 6.75343792, 5.64897424, 4.53259949, 2.48490665,\n",
       "       6.08221891, 5.433722  , 3.5       , 3.5       , 3.5       ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_URM_train = popularity_confidence(URM_train)\n",
    "\n",
    "C_URM_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the update rules for the user factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Update latent factors for a single user or item.\n",
    "\n",
    "Y = |n_interactions|x|n_factors|\n",
    "\n",
    "YtY =   |n_factors|x|n_factors|\n",
    "\n",
    "\n",
    "\n",
    "Latent factors ony of item/users for which an interaction exists in the interaction profile\n",
    "Y_interactions = Y[interaction_profile, :]\n",
    "\n",
    "Following the notation of the original paper we report the update rule for the Item factors (User factors are identical):\n",
    "* __Y__ are the item factors |n_items|x|n_factors|\n",
    "* __Cu__ is a diagonal matrix |n_interactions|x|n_interactions| with the user confidence for the observed items\n",
    "* __p(u)__ is a boolean vectors indexing only observed items. Here it will disappear as we already extract only the observed latent factors however, it will have an impact in the dimensions of the matrix, since it transforms Cu from a diagonal matrix to a row vector of 1 row and |n_interactions| columns\n",
    "\n",
    "$$(Yt*Cu*Y + reg*I)^-1 * Yt*Cu*profile$$ which can be decomposed as $$(YtY + Yt*(Cu-I)*Y + reg*I)^-1 * Yt*Cu*p(u)$$ \n",
    "\n",
    "* __A__ = (|n_interactions|x|n_factors|) dot (|n_interactions|x|n_interactions| ) dot (|n_interactions|x|n_factors| )\n",
    "  = |n_factors|x|n_factors|\n",
    "  \n",
    "We use an equivalent formulation (v * k.T).T which is much faster\n",
    "* __A__ = Y_interactions.T.dot(((interaction_confidence - 1) * Y_interactions.T).T)\n",
    "* __B__ = YtY + A + self.regularization_diagonal\n",
    "* __new factors__ = np.dot(np.linalg.inv(B), Y_interactions.T.dot(interaction_confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_row(interaction_profile, interaction_confidence, Y, YtY, regularization_diagonal):\n",
    "\n",
    "    Y_interactions = Y[interaction_profile, :]\n",
    "    \n",
    "    A = Y_interactions.T.dot(((interaction_confidence - 1) * Y_interactions.T).T)\n",
    "\n",
    "    B = YtY + A + regularization_diagonal\n",
    "\n",
    "    return np.dot(np.linalg.inv(B), Y_interactions.T.dot(interaction_confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0001, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.0001, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.0001, 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.0001, 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.0001, 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.0001, 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.0001, 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.0001,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.0001, 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.0001]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularization_coefficient = 1e-4\n",
    "\n",
    "regularization_diagonal = np.diag(regularization_coefficient * np.ones(num_factors))\n",
    "regularization_diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VV = n_factors x n_factors\n",
    "VV = item_factors.T.dot(item_factors)\n",
    "VV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_URM_train = linear_confidence_function(URM_train, alpha)\n",
    "\n",
    "start_pos = C_URM_train.indptr[user_id]\n",
    "end_pos = C_URM_train.indptr[user_id + 1]\n",
    "\n",
    "user_profile = C_URM_train.indices[start_pos:end_pos]\n",
    "user_confidence = C_URM_train.data[start_pos:end_pos]\n",
    "\n",
    "user_factors[user_id, :] = _update_row(user_profile, user_confidence, item_factors, VV, regularization_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply updates on the user item factors as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UU = n_factors x n_factors\n",
    "UU = user_factors.T.dot(user_factors)\n",
    "UU.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_URM_train_csc = C_URM_train.tocsc()\n",
    "\n",
    "start_pos = C_URM_train_csc.indptr[item_id]\n",
    "end_pos = C_URM_train_csc.indptr[item_id + 1]\n",
    "\n",
    "item_profile = C_URM_train_csc.indices[start_pos:end_pos]\n",
    "item_confidence = C_URM_train_csc.data[start_pos:end_pos]\n",
    "\n",
    "item_factors[item_id, :] = _update_row(item_profile, item_confidence, user_factors, UU, regularization_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put all together in a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69878 in 7.34 seconds. Users per second 9520.68\n",
      "Iteration 10681 in 10.11 seconds. Items per second 1056.52\n",
      "Epoch 1 complete in in 10.11 seconds\n",
      "Iteration 69878 in 6.94 seconds. Users per second 10071.77\n",
      "Iteration 10681 in 9.92 seconds. Items per second 1076.09\n",
      "Epoch 2 complete in in 9.92 seconds\n",
      "Iteration 69878 in 6.59 seconds. Users per second 10603.06\n",
      "Iteration 10681 in 9.62 seconds. Items per second 1110.51\n",
      "Epoch 3 complete in in 9.62 seconds\n",
      "Iteration 69878 in 7.29 seconds. Users per second 9585.91\n",
      "Iteration 10681 in 10.13 seconds. Items per second 1053.92\n",
      "Epoch 4 complete in in 10.13 seconds\n",
      "Iteration 69878 in 7.70 seconds. Users per second 9078.31\n",
      "Iteration 10681 in 10.49 seconds. Items per second 1017.99\n",
      "Epoch 5 complete in in 10.49 seconds\n",
      "Iteration 69878 in 6.60 seconds. Users per second 10590.22\n",
      "Iteration 10681 in 9.42 seconds. Items per second 1134.19\n",
      "Epoch 6 complete in in 9.42 seconds\n",
      "Iteration 69878 in 6.69 seconds. Users per second 10440.07\n",
      "Iteration 10681 in 9.26 seconds. Items per second 1153.53\n",
      "Epoch 7 complete in in 9.26 seconds\n",
      "Iteration 69878 in 6.80 seconds. Users per second 10280.49\n",
      "Iteration 10681 in 9.68 seconds. Items per second 1102.84\n",
      "Epoch 8 complete in in 9.68 seconds\n",
      "Iteration 69878 in 6.69 seconds. Users per second 10449.42\n",
      "Iteration 10681 in 9.48 seconds. Items per second 1126.31\n",
      "Epoch 9 complete in in 9.48 seconds\n",
      "Iteration 69878 in 6.68 seconds. Users per second 10458.79\n",
      "Iteration 10681 in 9.45 seconds. Items per second 1129.64\n",
      "Epoch 10 complete in in 9.45 seconds\n"
     ]
    }
   ],
   "source": [
    "C_URM_train_csc = C_URM_train.tocsc()\n",
    "\n",
    "num_factors = 10\n",
    "\n",
    "user_factors = np.random.random((n_users, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))\n",
    "\n",
    "\n",
    "for n_epoch in range(10):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for user_id in range(C_URM_train.shape[0]):\n",
    "\n",
    "        start_pos = C_URM_train.indptr[user_id]\n",
    "        end_pos = C_URM_train.indptr[user_id + 1]\n",
    "\n",
    "        user_profile = C_URM_train.indices[start_pos:end_pos]\n",
    "        user_confidence = C_URM_train.data[start_pos:end_pos]\n",
    "\n",
    "        user_factors[user_id, :] = _update_row(user_profile, user_confidence, item_factors, VV, regularization_diagonal)   \n",
    "\n",
    "        # Print some stats\n",
    "        if (user_id +1)% 100000 == 0 or user_id == C_URM_train.shape[0]-1:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            samples_per_second = user_id/elapsed_time\n",
    "            print(\"Iteration {} in {:.2f} seconds. Users per second {:.2f}\".format(user_id+1, elapsed_time, samples_per_second))\n",
    "\n",
    "\n",
    "    for item_id in range(C_URM_train.shape[1]):\n",
    "\n",
    "        start_pos = C_URM_train_csc.indptr[item_id]\n",
    "        end_pos = C_URM_train_csc.indptr[item_id + 1]\n",
    "\n",
    "        item_profile = C_URM_train_csc.indices[start_pos:end_pos]\n",
    "        item_confidence = C_URM_train_csc.data[start_pos:end_pos]\n",
    "\n",
    "        item_factors[item_id, :] = _update_row(item_profile, item_confidence, user_factors, UU, regularization_diagonal)    \n",
    "\n",
    "        # Print some stats\n",
    "        if (item_id +1)% 100000 == 0 or item_id == C_URM_train.shape[1]-1:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            samples_per_second = item_id/elapsed_time\n",
    "            print(\"Iteration {} in {:.2f} seconds. Items per second {:.2f}\".format(item_id+1, elapsed_time, samples_per_second))\n",
    "\n",
    "    total_epoch_time = time.time() - start_time  \n",
    "    print(\"Epoch {} complete in in {:.2f} seconds\".format(n_epoch+1, total_epoch_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long do we train such a model?\n",
    "\n",
    "* An epoch: a complete loop over all the train data\n",
    "* Usually you train for multiple epochs. Depending on the algorithm and data 10s or 100s of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time with the previous training speed is 94.54 seconds, or 1.58 minutes\n"
     ]
    }
   ],
   "source": [
    "estimated_seconds = total_epoch_time*10\n",
    "print(\"Estimated time with the previous training speed is {:.2f} seconds, or {:.2f} minutes\".format(estimated_seconds, estimated_seconds/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly: Computing a prediction for any given user or item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 17025\n",
    "item_id = 468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4238245031298778"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_rating = np.dot(user_factors[user_id,:], item_factors[item_id,:])\n",
    "predicted_rating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
